# Machine Learning & Crowd Simulation - Final Project: Learn and visualise representations for large data sets
### Project Summary
In our project, we will utilise the megaman library, which is designed for scalable manifold
learning. We will refer to the paper suggested by McQueen et al. and compare their
proposed technique with the performance of an autoencoder.
### Task 1/6 - Data Sets
Having read the aforementioned paper and potentially some supporting literature, we will
succinctly describe the crux of it. Finally we will describe data sets and the problem they
pose for our specific task.
### Task 2/6 - Implementation of the already existing algorithms
The existing algorithms, provided by the megaman package, will be explored. We will
familiarise ourselves with their functionalities, capabilities, and usage instructions. Two or
three fast representation algorithms from megaman, such as Isomap, or Local Linear
Embedding (LLE), will be selected and implemented.
### Task 3/6 - Training an Auto-Encoder
The focus will be shifted to implementing autoencoders. One or more variations of autoencoders, such as vanilla auto-encoders, sparse auto-encoders, denoising auto-encoders,
or variational auto-encoders (depending on the scope of your project and available
resources) will be selected and implemented.
### Task 4/6 - Testing Implementation on Large Self-Created Dataset
The implemented auto-encoders will be applied to a large self-created dataset. Its
performance (in terms of dimensionality reduction, feature extraction, and reconstruction
capabilities) will be evaluated and the results will be analysed.
### Task 5/6 - Testing Implementation on Large Dataset from the Paper
The autoencoder will be applied to the dataset from the paper and the experiments will be
replicated. The results will be compared and evaluated against the original results reported
in the paper.
Task 6/6 - Comparison of Results between the Datasets and Megaman Algorithms
Metrics, such as performance, accuracy, and efficiency, will be used to compare the
autoencoders to the megaman algorithms. Any differences and similarities observed, as
well as strengths and limitations will be discussed.
